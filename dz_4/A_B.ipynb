{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка работоспосбоности nltk, path- указка пути скачивания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\Slyexistence/nltk_data', 'c:\\\\Users\\\\Slyexistence\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\nltk_data', 'c:\\\\Users\\\\Slyexistence\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\share\\\\nltk_data', 'c:\\\\Users\\\\Slyexistence\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python312\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\Slyexistence\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data']\n",
      "['Пример', 'текста', 'для', 'проверки', 'токенизации']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:/Users/Slyexistence/AppData/Roaming/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:/Users/Slyexistence/AppData/Roaming/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "print(nltk.data.path)\n",
    "\n",
    "nltk.data.path.append('C:/Users/Slyexistence/nltk_data')\n",
    "nltk.download('punkt', download_dir='C:/Users/Slyexistence/AppData/Roaming/nltk_data')\n",
    "nltk.download('stopwords', download_dir='C:/Users/Slyexistence/AppData/Roaming/nltk_data')\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(\"Пример текста для проверки токенизации\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часть А"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Slyexistence\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "\n",
    "data = pd.read_csv(\"D:/ВУЗ/Атаева/Домашняя Работа/HomeWorkAnalyz/work_1/habr_articles.csv\")\n",
    "documents = data['Текст'].dropna().tolist()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "processed_docs = [preprocess_text(doc) for doc in documents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Темы, найденные LDA:\n",
      "Тема 1: ['linux', 'разработки', 'сегодня', 'разработчик', 'данных', 'таких', 'хабр', 'зовут', 'это', 'привет']\n",
      "Тема 2: ['также', 'управления', 'разработки', 'системы', 'это', 'данной', 'недавно', 'эта', 'статья', 'статье']\n",
      "Тема 3: ['python', 'назад', 'программирования', 'работы', 'статью', 'лет', 'пользователь', 'сделать', 'который', 'это']\n",
      "Тема 4: ['безопасности', 'также', 'могут', 'очень', 'время', 'пользователей', 'поэтому', 'которые', 'данных', 'это']\n",
      "Тема 5: ['зовут', 'поделиться', 'хочу', 'вами', 'расскажу', 'статье', 'всем', 'сегодня', 'хабр', 'привет']\n",
      "\n",
      "Темы, найденные LSA:\n",
      "Тема 1: ['данных', 'расскажу', 'компании', 'сегодня', 'статье', 'это', 'зовут', 'всем', 'хабр', 'привет']\n",
      "Тема 2: ['сегодня', 'каждый', 'которая', 'который', 'которые', 'например', 'статье', 'данных', 'это', 'пользователь']\n",
      "Тема 3: ['статья', 'лет', 'время', 'работы', 'также', 'который', 'которые', 'статье', 'данных', 'это']\n",
      "Тема 4: ['которых', 'расскажу', 'помощью', 'часть', 'статье', 'разработчик', 'компании', 'привет', 'зовут', 'всем']\n",
      "Тема 5: ['рассказать', 'сегодня', 'данной', 'эта', 'рассмотрим', 'которые', 'расскажу', 'помощью', 'статья', 'статье']\n",
      "\n",
      "Темы, найденные NMF:\n",
      "Тема 1: ['сервисов', 'занимаюсь', 'расскажу', 'python', 'связи', 'мтс', 'сегодня', 'зовут', 'привет', 'хабр']\n",
      "Тема 2: ['должны', 'сегодня', 'почему', 'стали', 'практически', 'которая', 'одна', 'каждый', 'например', 'пользователь']\n",
      "Тема 3: ['просто', 'поэтому', 'компании', 'всё', 'время', 'лет', 'работы', 'который', 'данных', 'это']\n",
      "Тема 4: ['александр', 'команде', 'года', 'связи', 'часть', 'разработчик', 'компании', 'зовут', 'привет', 'всем']\n",
      "Тема 5: ['статьи', 'эта', 'сегодня', 'также', 'рассмотрим', 'помощью', 'которые', 'расскажу', 'статья', 'статье']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Slyexistence\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1759: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, TruncatedSVD, NMF\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.9, min_df=10)\n",
    "dtm = vectorizer.fit_transform(processed_docs)\n",
    "\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda.fit(dtm)\n",
    "\n",
    "print(\"Темы, найденные LDA:\")\n",
    "for idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Тема {idx + 1}: {[vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]}\")\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.9, min_df=10)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_docs)\n",
    "\n",
    "lsa = TruncatedSVD(n_components=5, random_state=42)\n",
    "lsa.fit(tfidf_matrix)\n",
    "\n",
    "print(\"\\nТемы, найденные LSA:\")\n",
    "for idx, topic in enumerate(lsa.components_):\n",
    "    print(f\"Тема {idx + 1}: {[tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]}\")\n",
    "\n",
    "nmf = NMF(n_components=5, random_state=42)\n",
    "nmf.fit(tfidf_matrix)\n",
    "\n",
    "print(\"\\nТемы, найденные NMF:\")\n",
    "for idx, topic in enumerate(nmf.components_):\n",
    "    print(f\"Тема {idx + 1}: {[tfidf_vectorizer.get_feature_names_out()[i] for i in topic.argsort()[-10:]]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Упоминаемые персоны: {'татьяна ошуркова', 'игорь иванович', 'адаптирована ваши', 'константин', 'карл это', 'даша волкова', 'дмитрий рожков', 'дарья санькова', 'алексей зотов', 'екатерина шеленкова', 'александр aka bytehope', 'глеб михеев', 'александр михеев', 'александр карпов', 'романа сакутина', 'мария', 'антон комаров', 'алла работаю младшим', 'дмитрия медведева', 'юрием вашинко', 'джорджа буля', 'алана колмероэ', 'александра рябова', 'амир хотел', 'редактор хабра', 'даниил нейман', 'алексей карпенко', 'татьяна маркина', 'анжелика захарова', 'ефимов михаил', 'денис net backend', 'влюблён хираяма', 'дмитрий middlereactразработчик', 'бу', 'дмитрий фролов', 'артем мелехин', 'сергей щербаков', 'максим qa максилекте', 'сергей рыжков', 'кирилл', 'анастасия березовская', 'лекс фридман', 'александр патутинский', 'александр устюжанин', 'айо перевела', 'алексей жиряков', 'александр бобряков', 'дональд кнут мартин фаулер карлос буэно', 'филипенко', 'сергей рабинович', 'павел маслов', 'антон володченко', 'екатерина security vision', 'кличке эдес', 'вова', 'лифа бт', 'антон геннадий', 'фиче', 'екатерина flutterразработчик', 'джошуа эдес', 'вячеслав бенедичук', 'василина кузнецова', 'дмитрий селютин', 'константин константин', 'антон работаю команде', 'елена тутубалина', 'олег казаков', 'максим рубан', 'ирина', 'альберт эйнштейн', 'драил гальюн', 'александр иванченко', 'юрия панчула', 'яндекса команда', 'андрей арсений', 'нейрооптимизаторам рсубд', 'анна асабина', 'супераппа бизнеса', 'яндекса', 'десяток шаблонов', 'денди', 'артем валов', 'наконецто', 'виктор георгиевич сиротин', 'александр нилов', 'александр разрабатываю', 'настя швецова', 'роман щербаков', 'александра пилюгина', 'александр разберет', 'ооо телепорт', 'никита', 'сергей x5', 'алексей топчий', 'сергей баширов', 'роберта ковальски дэвида', 'михаил чайчук', 'юрьев артемий', 'факторе формирующем', 'кевин денг', 'вадим королев', 'паращенко', 'геймдева', 'лена работаю iosразработчиком kts', 'тысячу', 'максим flutterразработчик', 'карла симса', 'курт гёдель', 'вадим начинал карьеру', 'кен томпсона', 'олега чулакова', 'сергей востриков', 'алло', 'сергей вяльцев', 'настя эвелина', 'геннадий мухамедзянов', 'цех кипиа', 'полина кудрявцева', 'станислав ермохин', 'хираяма', 'павел бузин', 'какойто', 'сергей константинов', 'женя мельцайкин', 'павел слюсарь', 'зову', 'фотошопу базовым', 'дискорда', 'магнусом карлсеном яном', 'константин евтушенко', 'тимур нургалиев', 'авито опыт', 'корус консалтинг', 'андрей инженер', 'настя ищенко', 'фронтенда криптоните', 'андрей жадан______________________ростовнадону______________________2024', 'николай омётов', 'егор молчанов', 'сергей sdetспециалист itкомпании simbirsoft', 'дарина кухтина', 'алексей иванов', 'руслан абдуллин', 'дарио амодеи', 'павел мокеев', 'виктор пелевин чапаев', 'кирилл кирьянов', 'василий беляев', 'фудтехе агротехе', 'бэкапы', 'дмитрий лёвочкин', 'николай анисеня', 'иван клюев', 'фича баг', 'корутин android', 'артём хакимов разработчики', 'екатерина саяпина', 'предположим какоето', 'виктор сергеев', 'наташа бакалдина', 'андрей осипов', 'никита хилов', 'тимур напреев', 'ян акмеев', 'денис савран', 'денис веренцов', 'сергей лысанов', 'какоето', 'александр разработчик', 'адди османи', 'александр старший', 'эдес', 'прошлой статье', 'антон полухин', 'михраба квазикристаллов', 'иван леонов', 'александр леонов'}\n",
      "Географические названия: {'сша', 'европе', 'северной кореи', 'италии', 'китая', 'россия', 'москве', 'пошагово', 'репозитория', 'кнр', 'архитектуре', 'китае', 'австрии', 'китай', 'россию', 'россии', 'индонезии', 'вьетнам', 'вьетнаме', 'ес', 'российской федерации', 'рф', 'милане', 'евросоюз', 'токио', 'запада', 'фронтенда', 'лондон'}\n",
      "Названия организаций: {'kanso', 'mvt abтестированием api', 'cto', 'msdos', 'llm langchain', 'visual language models vlm', 'tms', 'idigital', 'modo', 'databases', 'rest api', 'business intelligence', 'vpnприложения', 'swordfish security', 'kafka streams', 'tsmc', 'айо', 'netflix', 'ispmanager', 'mlпроцессами', 'intellij idea community', 'biscoe dream torgersenзагрузим', 'scala nishtyaki', 'daniele polencic', 'waypoints', 'sso', 'dbaas postgresql', 'javascripttypescriptкод', 'dba', 'kojima productions', 'ркн', 'lukashininборисов', 'testdriven development', 'motherduck', 'godot', 'rdf', 'maven', 'angie', 'bi business intelligence', 'release notes', 'гк юзтех', 'intel', 'itразработке', 'confluence jira', 'prince of persia the lost crown', 'kali linux tails centos raspbian', 'nftables', 'acid', 'asml', 'nginx', 'аля', 'cloudru', 'openshift kubernetes k8s', 'sentinel', 'цп', 'тк', 'мтс true tech day', 'javaкоде', 'uplevel', 'delphi', 'antoshkka userver', 'orderbook', 'kaas', 'seaborn', 'opa rules', 'flutter team lead', 'data science', 'product owner', 'scriptfu', 'goblinrat', 'okko', 'e2eтесты', 'cloudru evolution', 'common table expressions cte', 'docker hub', 'ух', 'kion', 'вкс', 'ecommerce', 'google grpc', 'jira', 'dictionary dict', 'гис', 'coding machines', 'qaинженера', 'gimp', 'sony', 'javascript', 'т1', 'golang', 'dd planet', 'aptатак', 'ai', 'active directory', 'garbage collection in java the progress since jdk', 'kubernetes services', 'роскомнадзор', 'inversify', 'raft', 'ozon', 'whatsapp', 'excel', 'api amazon', 'smm', 'рфв', 'lowcode', 'gimp scriptfu', 'v4121', 'втб', 'api', 'vk', 'express', 'gpuсервер', 'shadowsocks vless vmess', 'problem management', 'твз', 'skillbox', 'vogue', 'cognitive ai systems airi', 'facepalm', 'compose', 'groq', 'ytsaurus', 'linux', 'pnfs', 'ип', 'prolog', 'youtube', 'bash', 'pt expert security center', 'sbashiro', 'nintendo', 'city bloxx', 'hello games', 'cms wordpress', 'round robin dns', 'yoloprice', 'friflex', 'oracle corporation', 'appleустройствам', 'securityfirst', 'biсистемы', 'рбк', 'adsl', 'bmc', 'miro notion', 'sap', 'firewalk studios', 'мтс exolve', 'postgresql postgresql 17 released', 'ibm pc клонов', 'unplugin', 'ea', 'userver', 'wal writeahead log', 'jetbrains', 'positive technologies', 'x5 group', 'mess with dns', 'perfexpert', 'nextjs', 'gitlab k8s', 'repl', 'control plane', 'welcome', 'rust', 'steve reisenberg', 'ssh telnet', 'llm', 'webkit chrome', 'spring', 'python', 'spacy', 'итд', 'redline meta', 'сми', 'ssh', 'graph retrievalaugmented generation graphrag', 'webpack vite esbuild', 'it компании', 'gpu', 'speechtotext', 'wifi', 'androidразработчик', 'hakaton', 'activesession', 'kubernetes', 'css', 'qfn quad flat nolead son small outline nolead dfn dual flat nolead qfp quad flat package', 'self storageскладе', 'rd cloudru', 'чём', 'cотрудники', 'google brain', 'yandexart', 'github', 'bac', 'kubernetesсервисов', 'interval', 'mastercard', 'matlab simulink amesim', 'prodigy', 'stl', 'ecom', 'product cult', 'ubisoft', 'meego maemo sailfishподобными', 'ciso', 'artemiyueaxлукашин', 'ab', 'convolutional neural networks cnn', 'r570e', 'idp', 'memoryaugmented models', 'pocketpair', 'ecb', 'surf', 'it talks', 'anthropic', 'emacs', 'huawei', 'совете безопасности', 'pcie', 'openidm', 'scala times', 'cohere cohere', 'jdk 21', 'effective technologies', 'ue', 'just for fun', 'pgauge', 'nse', 'nlp', 'usecase interactor', 'idx', 'stable', 'hello world undo', 'deep learning', 'emcee', 'ansible', 'penguins', 'securelist', 'usrsharegimp20scriptsscriptfuinit', 'linus linux', 'palworld', 'к2тех', 'flutterразработчика', 'seo', 'react native', 'keycloak', 'androidразработчик студии', 'vk cloud', 'fail2ban acl tarpit', 'chatgpt', 'authority', 'telegram mini app', 'user stories', 'selfhosted', 'hacktoria ron kaminsky', 'yadro', 'scatterplotперейдём', 'learnk8s', 'syntacore', 'мтс', 'webstorm', 'html django', 'microsoft teams', 'maxpatrol siem', 'nginx haproxy', 'machine learning operations mlops mlops', 'linuxузлами', 'cnn', 'elasticsearch opensearch', 'frontendconf', 'featuresliced design', 'spectr', 'википедии', 'bi', 'crm', 'tg', 'structured authentication config', 'vscode', 'selenium playwright', 'anonsudan', 'тд', 'intellij idea', 'aspnet', 'kotlin wrappers', 'java', 'phaseshifters', 'agima', 'threatbook', 'arrow', 'ото', 'vscode webstorm', 'dart', 'google', 'гк', 'graphrag', 'petr zapletal', 'vram', 'telegram', 'gravity ui', 'the internet archive', 'encrypted client hello ech cloudflare', 'postgresql', 'microsoft', 'jj'}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "# Функция для выделения сущностей\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    persons = set()\n",
    "    locations = set()\n",
    "    organizations = set()\n",
    "    \n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PER\":  # Имена персон\n",
    "            persons.add(ent.text)\n",
    "        elif ent.label_ == \"LOC\":  # Географические названия\n",
    "            locations.add(ent.text)\n",
    "        elif ent.label_ == \"ORG\":  # Организации\n",
    "            organizations.add(ent.text)\n",
    "    \n",
    "    return {\n",
    "        \"persons\": persons,\n",
    "        \"locations\": locations,\n",
    "        \"organizations\": organizations\n",
    "    }\n",
    "\n",
    "entities_per_document = [extract_entities(doc) for doc in processed_docs]\n",
    "\n",
    "all_persons = set()\n",
    "all_locations = set()\n",
    "all_organizations = set()\n",
    "\n",
    "for entities in entities_per_document:\n",
    "    all_persons.update(entities[\"persons\"])\n",
    "    all_locations.update(entities[\"locations\"])\n",
    "    all_organizations.update(entities[\"organizations\"])\n",
    "\n",
    "print(\"Упоминаемые персоны:\", all_persons)\n",
    "print(\"Географические названия:\", all_locations)\n",
    "print(\"Названия организаций:\", all_organizations)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
