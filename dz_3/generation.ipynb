{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 989 entries, 0 to 999\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   Название        989 non-null    object\n",
      " 1   Время чтения    989 non-null    object\n",
      " 2   Просмотры       989 non-null    object\n",
      " 3   Текст           989 non-null    object\n",
      " 4   Ключевые слова  989 non-null    object\n",
      " 5   Ссылка          989 non-null    object\n",
      "dtypes: object(6)\n",
      "memory usage: 54.1+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, GRU, LSTM, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "\n",
    "df = pd.read_csv(\"D:/ВУЗ/Атаева/Домашняя Работа/HomeWorkAnalyz/work_1/habr_articles.csv\")\n",
    "df =  df.dropna(subset=[\"Текст\"])\n",
    "df.info()\n",
    "text = df['Текст']\n",
    "input_sequences = []\n",
    "\n",
    "def clean(text):\n",
    "    text = re.sub(r'[!-,.«»:?;]', '', text.lower())\n",
    "    text = re.sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "text = text.apply(clean)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text)\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "\n",
    "\n",
    "for line in text:\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)\n",
    "\n",
    "max_sequence_length = max(len(seq) for seq in input_sequences)\n",
    "input_sequences = pad_sequences(input_sequences, \n",
    "                                maxlen=max_sequence_length, \n",
    "                                padding='pre')\n",
    "\n",
    "X, y = input_sequences[:, :-1], input_sequences[:, -1]\n",
    "y = tf.keras.utils.to_categorical(y, num_classes=total_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Нейросетки\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rnn = Sequential()\n",
    "model_rnn.add(Embedding(total_words, 50, input_length=max_sequence_length-1))\n",
    "model_rnn.add(SimpleRNN(100))\n",
    "model_rnn.add(Dense(total_words, activation='softmax'))\n",
    "model_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_rnn.fit(X, y, epochs=10, verbose=0)\n",
    "\n",
    "model_gru = Sequential()\n",
    "model_gru.add(Embedding(total_words, 50, input_length=max_sequence_length-1))\n",
    "model_gru.add(GRU(100))\n",
    "model_gru.add(Dense(total_words, activation='softmax'))\n",
    "model_gru.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_gru.fit(X, y, epochs=10, verbose=0)\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(total_words, 50, input_length=max_sequence_length-1))\n",
    "model_lstm.add(LSTM(100))\n",
    "model_lstm.add(Dense(total_words, activation='softmax'))\n",
    "model_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_lstm.fit(X, y, epochs=10, verbose=0)\n",
    "\n",
    "def generate_text(seed_text, model, max_sequence_len, num_words):\n",
    "    for _ in range(num_words):\n",
    "        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
    "        predicted = pd.argmax(model.predict(token_list), axis=-1)\n",
    "        output_word = \"\"\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == predicted:\n",
    "                output_word = word\n",
    "                break\n",
    "        seed_text += \" \" + output_word\n",
    "    return seed_text\n",
    "\n",
    "generated_text_rnn = generate_text(\"Это\", model_rnn, max_sequence_length, num_words=10)\n",
    "generated_text_gru = generate_text(\"Это\", model_gru, max_sequence_length, num_words=10)\n",
    "generated_text_lstm = generate_text(\"Это\", model_lstm, max_sequence_length, num_words=10)\n",
    "\n",
    "print(\"Сгенерированный текст (SimpleRNN):\", generated_text_rnn)\n",
    "print(\"Сгенерированный текст (GRU):\", generated_text_gru)\n",
    "print(\"Сгенерированный текст (LSTM):\", generated_text_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Статистическая модель марковская цепь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Текст 1:\n",
      "бонус в конце октября текущего 2024 года мы в airi и занимаюсь созданием и продвижением сервисов на основе индивидуального субъективного опыта человека создаются новые условия для совместного роста человека и человеческих отношений особенности которой эксплуатируются самым наглым образом у всех на виду мошенники что-то поняли о людях раньше всех остальных и нам\n",
      "\n",
      "Текст 2:\n",
      "комментариях появились возражения мол сложная аналитика не всегда отражает содержимое файла с неформатированным текстом это байтовые коды закодированные в таких изысканиях понятна любому кто пытался добиться от chatgpt точного ответа на конкретный вопрос подобрать литературу для курсовой необходимо было создать клиент - серверное приложение и решено было создать клиент - серверное приложение\n",
      "\n",
      "Текст 3:\n",
      "данных обсуждали в течение последних богатых событиями лет одна вещь не перестаёт пробиваться сквозь любой новостной фон регулярные истории людей отдающих все свои и иногда заёмные деньги просто потому что мы уже оказывается некоторым образом давно знакомы позвольте перейти к статье\n",
      "\n",
      "Текст 4:\n",
      "layerzero это неизменяемый устойчивый к цензуре и не всем надо\n",
      "\n",
      "Текст 5:\n",
      "совсем недавно разработчики применяющие сканер образов trivy столкнулись с тем что система становилась менее отзывчивой особенно при запуске нескольких контейнеров одновременно высокая нагрузка на cpu и gpu\n",
      "\n",
      "Текст 6:\n",
      "оставляет желать лучшего иначе как объяснить тот хаос и стресс который каждый из которых их собирают изготавливают на заводе при этом время эксплуатации уязвимости после ее раскрытия сокращается существуют различные варианты попыток защиты\\сокрытия сервисов от любопытных глаз основные использование нестандартного порта fail2ban acl и tarpit и их обработки по специальным алгоритмам что\n",
      "\n",
      "Текст 7:\n",
      "67-мая страница на черно-оранжевом сайте новое исследование проливает свет на эдакие архетипы нетраннера которые показывают как именно сделать это на самом деле больше так как хоть на данный момент они используются почти во всех фундаментальных моделях от тех что под nda обычно кандидаты задают технические вопросы по стеку пайплайнам иногда пытаются задать\n",
      "\n",
      "Текст 8:\n",
      "тк скорее всего у них читалка наверное над её созданием трудится целая команда в тот момент ограничены внутренней сетью но на то мы и люди специалисты отобранные после нескольких этапов тестирований и обученные на настоящих проектах\n",
      "\n",
      "Текст 9:\n",
      "речь заходит о динамическом управлении памятью в языке программирования python получил широкую популярность среди разработчиков благодаря богатому функционалу и гибкости однако как и не только помогает создавать коллекции дизайнерам но и уверенного знания sql для решения задач различной сложности несмотря на бурное развитие подводных дронов по науке - автономные и управляемые необитаемые\n",
      "\n",
      "Текст 10:\n",
      "об автоматизации оборудования juniper в качестве первичного источника данных пользователей может выступать служба каталогов ldap реляционная база данных key‑value‑пары которой объединяются в привычные пользователям реляционных субд таблицы в ytsaurus в них можно хранить огромные массивы данных при этом безболезненно внедрить новую систему на рабочие места и обучить сотрудников\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "df = pd.read_csv(\"D:/ВУЗ/Атаева/Домашняя Работа/HomeWorkAnalyz/work_1/habr_articles.csv\")\n",
    "df = df.dropna(subset=[\"Текст\"])\n",
    "df = df.drop(columns=['Название', 'Время чтения', 'Просмотры', 'Ключевые слова', 'Ссылка'])\n",
    "text = df['Текст']\n",
    "\n",
    "def clean(text):\n",
    "    text = re.sub(r'[!-,.«»:?;—]', '', text.lower())\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "texts = text.apply(clean)\n",
    "\n",
    "def build_markov_chain(texts, n=2):\n",
    "    chain = defaultdict(list)\n",
    "    for text in texts:\n",
    "        tokens = text.split()\n",
    "        for i in range(len(tokens) - n + 1):\n",
    "            key = tuple(tokens[i:i + n - 1])  \n",
    "            next_word = tokens[i + n - 1]   \n",
    "            chain[key].append(next_word)\n",
    "    return chain\n",
    "\n",
    "def generate_text_markov(chain, n=2, start_text='', max_words=50):\n",
    "    if start_text:\n",
    "        current_state = tuple(start_text.split()[:n-1])\n",
    "    else:\n",
    "        current_state = random.choice(list(chain.keys())) \n",
    "    \n",
    "    generated_text = list(current_state)\n",
    "    \n",
    "    for _ in range(max_words):\n",
    "        next_words = chain.get(current_state, [])\n",
    "        if not next_words:\n",
    "            break\n",
    "        next_word = random.choice(next_words)\n",
    "        generated_text.append(next_word)\n",
    "        current_state = tuple(generated_text[-(n-1):]) \n",
    "    \n",
    "    return ' '.join(generated_text)\n",
    "\n",
    "def generate_multiple_texts(chain, n=2, max_words=50, number_offers=10):\n",
    "    texts = []\n",
    "    for _ in range(number_offers):\n",
    "        generated_text = generate_text_markov(chain, n=n, start_text='', max_words=max_words)\n",
    "        texts.append(generated_text)\n",
    "    return texts\n",
    "\n",
    "\n",
    "n = 3  \n",
    "markov_chain = build_markov_chain(texts, n)\n",
    "generated_texts = generate_multiple_texts(markov_chain, n=n, max_words=50, number_offers=10)\n",
    "\n",
    "for i, t in enumerate(generated_texts, 1):\n",
    "    print(f\"Текст {i}:\\n{t}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
